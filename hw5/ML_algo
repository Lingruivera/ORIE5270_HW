{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>ORIE5270 HW5: LSTM VS GRU</center></h1>\n",
    "<h3><center>Lingrui Li(ll829)</center></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data\n",
    "\n",
    "Set the seed, define the `Fields` and get the train/valid/test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "import torch\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "import random\n",
    "\n",
    "#set random seed for checking results\n",
    "SEED = 1234\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "\n",
    "#Create Field objects for preprocessing. The Field sets rules for preprocessing the input text.\n",
    "#The tokenize function is used to tokenize strings using this field into sequential examples. It takes default value: str.split.\n",
    "#Here “spacy” is specified, then the SpaCy English tokenizer is used.\n",
    "TEXT = data.Field(tokenize='spacy')\n",
    "\n",
    "#LABELField is a subsection of Field.\n",
    "#LABEL Data should be processed as the float tensor type.\n",
    "LABEL = data.LabelField(tensor_type=torch.FloatTensor)\n",
    "\n",
    "\n",
    "#Load and Splits the torchtext.datasets objects, IMDB dataset, as traning set and test test.\n",
    "#Use the predifined rules to preprocess Field.\n",
    "train, test = datasets.IMDB.splits(TEXT, LABEL)\n",
    "\n",
    "#Split the traning set into training set and validation set, the random seed used for shuffling is the predefined seed.\n",
    "train, valid = train.split(random_state=random.seed(SEED))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construct the Vocab object from the TEXT field for the training datasets using the top 25000 most common words.\n",
    "#Initialize word embeddings with pre-trained vectors where words that appear in similar contexts appear nearby in this vector space.\n",
    "#Use glove to calculate the vectors which are 100-dimensional vectors trained on 6 billion tokens.\n",
    "TEXT.build_vocab(train, max_size=25000, vectors=\"glove.6B.100d\")\n",
    "\n",
    "#Construct the Vocab object from the LABEL field for the training datasets.\n",
    "LABEL.build_vocab(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the iterators.  \n",
    "During training, we'll be using a special kind of Iterator, called the **BucketIterator**.  \n",
    "The BucketIterator groups sequences of similar lengths together for each batch to minimize padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the number of examples in the batch.\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "#Load Examples from The Dataset object: train, valid and test\n",
    "\n",
    "#The number of examples in each batch is the predefined BATCH_SIZE(= 64)\n",
    "\n",
    "#sort_key is a key to use for sorting examples in order to batch together examples with similar lengths and minimize padding\n",
    "#Here, the length of each sentence is used to sort and group data.\n",
    "\n",
    "#repeat =False, the iterator is not repeated for multiple epochs.\n",
    "\n",
    "#The iterator returns a torchtext.data.Batch datatype, contains batches of examples where each example is of a similar length.\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train, valid, test), \n",
    "    batch_size=BATCH_SIZE, \n",
    "    sort_key=lambda x: len(x.text), \n",
    "    repeat=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create LSTM and GRU classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the neutral network package\n",
    "import torch.nn as nn\n",
    "\n",
    "class RNN_LSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout):\n",
    "        \"\"\"\n",
    "        obj:\n",
    "        Define initial parameters and methods for LSTM\n",
    "        \n",
    "        paras:\n",
    "        vocab_size - int; the dimention of the one-hot vector, length of TEXT.vocab.\n",
    "        embedding_dim - int; the dimension of the dense word vector.\n",
    "        hidden_dim - int; the size of hidden states.\n",
    "        output_dim - int; the number of classes, in our case, the output is 0 or 1, so the output_dim is set to 1.\n",
    "        n_layers - int; adding additional layers on the RNN.\n",
    "        bidirectional - bool; if True, implementing bidirectional RNN.\n",
    "        dropout - float; drop out probability, the paramter of regularization to avoid overfitting, randomly dropping out (setting to 0) neurons during a forward pass.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, bidirectional=bidirectional, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim*2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        obj:\n",
    "        Defines the forwarding process between each step.\n",
    "        \n",
    "        paras:\n",
    "        x: [sent len, batch size].\n",
    "        \n",
    "        returns:\n",
    "        The output and a tuple of the final hidden state and the final cell state.\n",
    "        \"\"\"\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "        \n",
    "        #embedded = [sent len, batch size, emb dim]\n",
    "        \n",
    "        output, (hidden, cell) = self.rnn(embedded)\n",
    "        \n",
    "        #output = [sent len, batch size, hid dim * num directions]\n",
    "        #hidden = [num layers * num directions, batch size, hid. dim]\n",
    "        #cell = [num layers * num directions, batch size, hid. dim]\n",
    "        \n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
    "                \n",
    "        #hidden [batch size, hid. dim * num directions]\n",
    "            \n",
    "        return self.fc(hidden.squeeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. GRU Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN_GRU(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout):\n",
    "        \"\"\"\n",
    "        obj:\n",
    "        Define initial parameters and methods for GRU\n",
    "        \n",
    "        paras:\n",
    "        vocab_size: int; the dimention of the one-hot vector, length of TEXT.vocab.\n",
    "        embedding_dim: int; the dimension of the dense word vector.\n",
    "        hidden_dim: int; the size of hidden states.\n",
    "        output_dim: int; the number of classes, in our case, the output is 0 or 1, so the output_dim is set to 1.\n",
    "        n_layers: int; adding additional layers on the RNN.\n",
    "        bidirectional: bool; if True, implementing bidirectional RNN.\n",
    "        dropout: float; drop out probability, the paramter of regularization to avoid overfitting, randomly dropping out (setting to 0) neurons during a forward pass.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.GRU(embedding_dim, hidden_dim, num_layers=n_layers, bidirectional=bidirectional, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim*2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        obj:\n",
    "        Defines the forwarding process between each step.\n",
    "        \n",
    "        paras:\n",
    "        x: [sent len, batch size].\n",
    "        \n",
    "        returns:\n",
    "        The output and the final hidden state.\n",
    "        \"\"\"\n",
    "        \n",
    "        #x = [sent len, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "        \n",
    "        #embedded = [sent len, batch size, emb dim]\n",
    "        \n",
    "        output, hidden = self.rnn(embedded)\n",
    "        \n",
    "        #output = [sent len, batch size, hid dim * num directions]\n",
    "        #hidden = [num layers * num directions, batch size, hid. dim]\n",
    "        \n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
    "                \n",
    "        #hidden [batch size, hid. dim * num directions]\n",
    "            \n",
    "        return self.fc(hidden.squeeze(0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set input parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define input parameters for the LSTM model and the GRU model\n",
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an instance for the LSTM model and the GRU model with input paramters\n",
    "model_LSTM = RNN_LSTM(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT)\n",
    "model_GRU = RNN_GRU(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check EMBEDDING_DIM of pre-trained GloVe vectors loaded earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25002, 100])\n"
     ]
    }
   ],
   "source": [
    "#Retrieve the embeddings from the field's vocab, and ensure they're the correct size, [vocab size, embedding dim]\n",
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "\n",
    "print(pretrained_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace the initial weights of the embedding layer with the pre-trained embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
       "        ...,\n",
       "        [-0.4096, -0.5753,  0.1126,  ...,  0.4092,  0.1856,  0.1066],\n",
       "        [ 0.2110, -0.2472,  0.6508,  ..., -0.1627,  0.4507, -1.1627],\n",
       "        [-0.2379, -0.1095,  0.4314,  ...,  0.6665,  0.3200,  0.8872]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#copying the pre-trained word embeddings we loaded earlier into the embedding layer of the LSTM model.\n",
    "model_LSTM.embedding.weight.data.copy_(pretrained_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
       "        ...,\n",
       "        [-0.4096, -0.5753,  0.1126,  ...,  0.4092,  0.1856,  0.1066],\n",
       "        [ 0.2110, -0.2472,  0.6508,  ..., -0.1627,  0.4507, -1.1627],\n",
       "        [-0.2379, -0.1095,  0.4314,  ...,  0.6665,  0.3200,  0.8872]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#copying the pre-trained word embeddings we loaded earlier into the embedding layer of the GRU model.\n",
    "model_GRU.embedding.weight.data.copy_(pretrained_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "#Choose optimizer Adam that adapts the learning rate for each parameter\n",
    "optimizer_LSTM = optim.Adam(model_LSTM.parameters())\n",
    "optimizer_GRU = optim.Adam(model_GRU.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define criterion for training the model and place to run the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the loss function as the BCEWithLogitsLoss \n",
    "#BCEWithLogitsLoss  = One Sigmoid Layer + BCELoss(solved numerically unstable problem)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "#Place the model and criterion on the GPU if available, else on CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#Place model on the GPU if available, else on CPU\n",
    "model_LSTM = model_LSTM.to(device)\n",
    "model_GRU = model_GRU.to(device)\n",
    "\n",
    "#Place the criterion on the GPU if available, else on CPU\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function to cumpute prediction accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import convolution functions\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    obj: compute prediction accuracy \n",
    "    \n",
    "    paras:\n",
    "    preds: predictions\n",
    "    y: actual labels\n",
    "    \n",
    "    returns: \n",
    "    accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    #Applies the element-wise function Sigmoid() on predictions .\n",
    "    #Round function values to the closest integer which is 0 or 1.\n",
    "    rounded_preds = torch.round(F.sigmoid(preds))\n",
    "    \n",
    "    #Compute the number of accurate predictions when rounded prediction == true label.\n",
    "    #Convert the result into float for division .\n",
    "    correct = (rounded_preds == y).float()\n",
    "    \n",
    "    #Compute the rate of accuracy.\n",
    "    acc = correct.sum()/len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the function to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \"\"\"\n",
    "    obj: \n",
    "    train the model and compute the traning scores\n",
    "    \n",
    "    paras:\n",
    "    model: model to be trained\n",
    "    iterator: a torchtext.data.Batch datatype\n",
    "    optimizer: optimizer to be used\n",
    "    criterion: model selection criterion\n",
    "    \n",
    "    returns:\n",
    "    The traning loss score and accuracy score.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    #initialize variables for computing traning results\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    #All the modules are initialized to train mode, it does dropout and normalize batch.\n",
    "    model.train()\n",
    "    \n",
    "    #Iterates over all batches in the iterator.\n",
    "    for batch in iterator:\n",
    "        \n",
    "        #Set gradients to be Zero.\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #Feed the batch text into the model.\n",
    "        #Remove dimensions of size 1 from the shape of the predictions which are size [batch size, 1].\n",
    "        predictions = model(batch.text).squeeze(1)\n",
    "        \n",
    "        #Feed predictions and true labels, calculate the loss. \n",
    "        loss = criterion(predictions, batch.label)\n",
    "        \n",
    "        #Feed predictions and true labels, calculate the accuracy. \n",
    "        acc = binary_accuracy(predictions, batch.label)\n",
    "        \n",
    "        #Calculate the gradients of parameters.\n",
    "        loss.backward()\n",
    "        \n",
    "        #Use the optimizer to update parameters. \n",
    "        optimizer.step()\n",
    "        \n",
    "        #Add the loss and accuracy for each batch\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the function to test the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \"\"\"\n",
    "    obj: \n",
    "    test the model and compute the test scores\n",
    "    \n",
    "    paras:\n",
    "    model: model to be trained\n",
    "    iterator: a torchtext.data.Batch datatype\n",
    "    criterion: model selection criterion\n",
    "    \n",
    "    returns:\n",
    "    The test loss score and accuracy score.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    #initialize variables for computing traning results\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    #All the modules are initialized to evaluation mode, it doesn't do dropout and use running mean and running var.\n",
    "    model.eval()\n",
    "    \n",
    "    #Deactivate the autograd engine. \n",
    "    #Reduce memory usage and speed up computations but you won’t be able to backprop. \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "            \n",
    "            #Feed the batch text into the model.\n",
    "            #Remove dimensions of size 1 from the shape of the predictions which are size [batch size, 1].\n",
    "            predictions = model(batch.text).squeeze(1)\n",
    "            \n",
    "            #Feed predictions and true labels, calculate the loss. \n",
    "            loss = criterion(predictions, batch.label)\n",
    "            \n",
    "            #Feed predictions and true labels, calculate the accuracy. \n",
    "            acc = binary_accuracy(predictions, batch.label)\n",
    "            \n",
    "            #Add the loss and accuracy for each batch\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train LSTM and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1006: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torchtext/data/field.py:322: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  return Variable(arr, volatile=not train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Train Loss: 0.686, Train Acc: 53.95%, Val. Loss: 0.666, Val. Acc: 59.26%\n",
      "Epoch: 02, Train Loss: 0.696, Train Acc: 52.43%, Val. Loss: 0.686, Val. Acc: 57.92%\n",
      "Epoch: 03, Train Loss: 0.677, Train Acc: 58.17%, Val. Loss: 0.671, Val. Acc: 58.49%\n",
      "Epoch: 04, Train Loss: 0.603, Train Acc: 67.36%, Val. Loss: 0.567, Val. Acc: 69.89%\n",
      "Epoch: 05, Train Loss: 0.447, Train Acc: 80.16%, Val. Loss: 0.461, Val. Acc: 80.33%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 5\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    train_loss, train_acc = train(model_LSTM, train_iterator, optimizer_LSTM, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model_LSTM, valid_iterator, criterion)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc*100:.2f}%, Val. Loss: {valid_loss:.3f}, Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torchtext/data/field.py:322: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  return Variable(arr, volatile=not train)\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1006: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.503, Test Acc: 77.65%\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = evaluate(model_LSTM, test_iterator, criterion)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f}, Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train GRU and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1006: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torchtext/data/field.py:322: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  return Variable(arr, volatile=not train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Train Loss: 0.691, Train Acc: 54.30%, Val. Loss: 0.652, Val. Acc: 61.96%\n",
      "Epoch: 02, Train Loss: 0.417, Train Acc: 81.17%, Val. Loss: 0.300, Val. Acc: 88.78%\n",
      "Epoch: 03, Train Loss: 0.239, Train Acc: 90.99%, Val. Loss: 0.257, Val. Acc: 90.04%\n",
      "Epoch: 04, Train Loss: 0.164, Train Acc: 93.84%, Val. Loss: 0.304, Val. Acc: 88.00%\n",
      "Epoch: 05, Train Loss: 0.125, Train Acc: 95.47%, Val. Loss: 0.304, Val. Acc: 89.75%\n"
     ]
    }
   ],
   "source": [
    "# Train the model through multiple epochs, an epoch being a complete pass through all examples in the split.\n",
    "N_EPOCHS = 5\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    train_loss_GRU, train_acc_GRU = train(model_GRU, train_iterator, optimizer_GRU, criterion)\n",
    "    valid_loss_GRU, valid_acc_GRU = evaluate(model_GRU, valid_iterator, criterion)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss_GRU:.3f}, Train Acc: {train_acc_GRU*100:.2f}%, Val. Loss: {valid_loss_GRU:.3f}, Val. Acc: {valid_acc_GRU*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torchtext/data/field.py:322: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  return Variable(arr, volatile=not train)\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1006: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.340, Test Acc: 88.30%\n"
     ]
    }
   ],
   "source": [
    "test_loss_GRU, test_acc_GRU = evaluate(model_GRU, test_iterator, criterion)\n",
    "\n",
    "print(f'Test Loss: {test_loss_GRU:.3f}, Test Acc: {test_acc_GRU*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Input\n",
    "\n",
    "We can now use our model to predict the sentiment of any sentence we give it. As it has been trained on movie reviews, the sentences provided should also be movie reviews.\n",
    "\n",
    "Our `predict_sentiment` function does a few things:\n",
    "- tokenizes the sentence, i.e. splits it from a raw string into a list of tokens\n",
    "- indexes the tokens by converting them into their integer representation from our vocabulary\n",
    "- converts the indexes, which are a Python list into a PyTorch tensor\n",
    "- add a batch dimension by `unsqueeze`ing \n",
    "- squashes the output prediction from a real number between 0 and 1 with the `sigmoid` function\n",
    "- converts the tensor holding a single value into an integer with the `item()` method\n",
    "\n",
    "We are expecting reviews with a negative sentiment to return a value close to 0 and positive reviews to return a value close to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "def predict_sentiment_LSTM(sentence):\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(1)\n",
    "    prediction = F.sigmoid(model_LSTM(tensor))\n",
    "    return prediction.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example negative review..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1006: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.14019495248794556"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment_LSTM(\"This film is terrible\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example positive review..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1006: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7670667767524719"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment_LSTM(\"This film is great\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "def predict_sentiment_GRU(sentence):\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(1)\n",
    "    prediction = F.sigmoid(model_GRU(tensor))\n",
    "    return prediction.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example negative review..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1006: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.35686933994293213"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment_GRU(\"This film is terrible\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example positive review..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py:1006: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9262372255325317"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment_GRU(\"This film is great\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison: LSTM VS GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the training results:  \n",
    "The LSTM model and the GRU model achieve similar traning scores and validation scores.\n",
    "\n",
    "From the test results:  \n",
    "The test scores are silimiar to the traning scores for both models. Therefore, they are not overfitting.  \n",
    "The GRU model achieves slightly higher test accuracy than the LSTM model.\n",
    "\n",
    "From the model model implemention:\n",
    "Since we are expecting the reviews with a negative sentiment to return a value close to 0 and positive reviews to return a value close to 1, the GRU produces better predictions than the LSTM model.\n",
    "\n",
    "However, since the GRU model is simpler than standard LSTM models, while achieving similar accuracy, we prefer GRU model more than the standard LSTM models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:python3]",
   "language": "python",
   "name": "conda-env-python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
